import os
import re
from urllib.parse import urljoin

import requests


def request(url):
    try:
        return requests.get('https://' + url)
    except requests.exceptions.ConnectionError:
        pass


def extract_url(url):
    response = requests.get(url)
    return re.findall('(?:href=")(.*?)"', str(response.content))


class Crawler:
    def __init__(self, target_url):
        self.target_url = target_url

    def discover_domains(self):
        subdomain_file = os.path.abspath(os.path.dirname(os.path.abspath(__file__))) + '\\data\\subdomains.txt'
        with open(subdomain_file, 'r') as subs:
            for line in subs:
                domain = line.strip()
                sub_url = domain + '.' + self.target_url
                response = request(sub_url)

                if response:
                    print('Discovered subdomain --> ' + sub_url)
                else:
                    pass

    def crawl(self, url):
        if 'http' not in url:
            url = 'https://' + url
        href_links = extract_url(url)

        for link in href_links:
            link = urljoin(self.target_url, link)

            if '#' in link:
                link = link.split('#')[0]

            if self.target_url in link and link not in self.target_url:
                print(link)
                self.crawl(link)

    def discover_hidden_paths(self):
        hidden_file = os.path.abspath(os.path.dirname(os.path.abspath(__file__))) + '\\data\\hidden.txt'
        with open(hidden_file, 'r') as hidden:
            for line in hidden:
                path = line.strip()
                hidden_url = self.target_url + '/' + path
                response = request(hidden_url)

                if response:
                    print('Discovered file or dir --> ' + hidden_url)
                else:
                    pass
