import os
import re
import requests
from urllib.parse import urljoin


def request(url):
    try:
        return requests.get('https://' + url)
    except requests.exceptions.ConnectionError:
        pass


class Scanner:
    def __init__(self, target_url):
        self.target_url = target_url
        self.session = requests.Session()

    def extract_links(self, url):
        response = self.session.get(url)
        return re.findall('(?:href=")(.*?)"', str(response.content))

    def crawl(self, url):
        if 'http' not in url:
            url = 'https://' + url
        href_links = extract_links(url)

        for link in href_links:
            link = urljoin(self.target_url, link)

            if '#' in link:
                link = link.split('#')[0]

            if self.target_url in link and link not in self.target_url:
                print(link)
                self.crawl(link)
